## Nex, we will do the mapping.
##As a first step, we will map the files to a reference hybrid genome, and then sort and index them.
## In the second step, we will compute size distributions. In a third step, we will filter (exclude) duplicates
## See, the script below.
## We need a new folder "05.BAM" and within this folder, a folder called "logs", a folder called "stats", and a folder called "nodupl".
## We will use the same sample file as initially created

#!/bin/bash -l
#SBATCH -J map
#SBATCH -o /scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/05.BAM/logs/map_%j.out
#SBATCH -e /scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/05.BAM/logs/map_%j.err
#SBATCH --account=project_2009316
#SBATCH -t 24:00:00
#SBATCH -p small
#SBATCH --array=1-18
#SBATCH --ntasks 8
#SBATCH --mem=16G
#SBATCH --mail-type=START,FAIL,END

module load biokit
module load r-env

cd /scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008

REFPATH=/scratch/project_2009316/refGenome       
TRIMPATH=/scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/04.TrimmedData
TMPBAMPATH=/scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/temp
BAMPATH=/scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/05.BAM

file=$(sed -n "$SLURM_ARRAY_TASK_ID"p name.list)
shortfile=${file}

echo "###### STARTING file: $file"

###
### 1. Map, sort & index ------------------------------------------------------
###

echo "###### MAPPING"

bwa mem -t8 $REFPATH/Formica_hybrid_v1_wFhyb_Sapis.fa $TRIMPATH/$file"_1.trim.pair.fq.gz" $TRIMPATH/$file"_2.trim.pair.fq.gz" | samtools sort -@8 -m512M -o $TMPBAMPATH/${shortfile}".bam" -

samtools index -@4 $TMPBAMPATH/${shortfile}".bam"

###
### 2. Compute insert size distribution ---------------------------------------
###

echo "###### COLLECTING INSERT SIZES"

picard16 CollectInsertSizeMetrics \
I=$TMPBAMPATH/${shortfile}".bam" \
O=$BAMPATH/stats/${shortfile}"_insert_size_metrics.txt" \
H=$BAMPATH/stats/${shortfile}"_insert_size_hist.pdf"



###
### 3. Filter duplicates ----------------------------------------------------
###

echo "###### FILTERING DUPLICATES"

picard16 MarkDuplicates \
I=$TMPBAMPATH/${shortfile}".bam" \
O=$BAMPATH/nodupl/${shortfile}"_nodupl.bam" \
M=$BAMPATH/nodupl/stats/${shortfile}"_dupl_metrics.txt" \
REMOVE_DUPLICATES=T TMP_DIR=$TMPBAMPATH


###
### 4. Index and compute stats ------------------------------------------------
###

echo "###### INDEX & GET FLAGSTATS"

samtools index -@4 $BAMPATH/nodupl/${shortfile}"_nodupl.bam" && \
samtools flagstat -@4 $BAMPATH/nodupl/${shortfile}"_nodupl.bam" > $BAMPATH/nodupl/stats/${shortfile}"_nodupl.flagstat" && \
samtools idxstats -@4 $BAMPATH/nodupl/${shortfile}"_nodupl.bam" > $BAMPATH/nodupl/stats/${shortfile}"_nodupl.idxstat"

echo "###### DONE! sample ID: $file"

### This is the end.



##################################################################
## Below, we calculate the mean insert sizes per sample directly on the cluster using sinteractive.

sinteractive --account project_2009316 --mem 8000  # requests an interactive job on a compute node
## Collect mean insert sizes per sample  --------------------
cd /scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/05.BAM/stats

for i in *_insert_size_metrics.txt; do echo "$i"; awk '/^MEDIAN_INSERT_SIZE/ {getline; print $6}' "$i"; done > all.mean_insert_sizes.tmp

grep -v 'txt' all.mean_insert_sizes.tmp > tmp1
grep 'txt' all.mean_insert_sizes.tmp | perl -npe 's/_insert_size_metrics.txt//' > tmp2
paste tmp2 tmp1 > all.mean_insert_sizes.txt ; rm *tmp*

## Download the data to the local machine
scp krapfpat@puhti.csc.fi:/scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/05.BAM/stats/all.mean_insert_sizes.txt .
## Locally, check in your favourite editor the inser size

####### NOTE: The output txt-file contains the inser sizes per sample, e.g.
### BBE1_MKDN250052002-1A_235LJ3LT4_L5       292.010742
### The standard Illumina paired-end insert size is 150-300
### 300â€“600 bp	would be a large-fragment library
### <150 bp indicated degraded DNA or over-fragmentation
### >600 bp	unusual, long-fragment library

################
### Collect mapping rates per sample  --------------------
cd /scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/05.BAM/nodupl/stats

for i in *_nodupl.flagstat
 do echo $i ; awk 'FNR == 7' $i
done > all.mapping_rates.tmp

grep -v 'flagstat' all.mapping_rates.tmp > tmp1
grep 'flagstat' all.mapping_rates.tmp | perl -npe 's/_nodupl.flagstat//' > tmp2
paste tmp2 tmp1 > all.mapping_rates.txt ; rm *tmp*

## Download the mapping rates per sample and check them in your favourite editor.
scp krapfpat@puhti.csc.fi:/scratch/project_2009316/DutchSamples/X204SC23115958-Z01-F008/05.BAM/nodupl/stats/all.mapping_rates.txt .

####### NOTE: The output txt-file contains the mapping rates per sample, e.g.
### BBE1_MKDN250052002-1A_235LJ3LT4_L5       63830504 + 0 mapped (99.30%  NA)
### It shows that the 99.30% of all reads in that BAM file have been aligned to the genome.
### The remaining ~0.70% were unmapped, meaning they did not match the reference sufficiently (due to low quality, contamination, divergence, etc.).

